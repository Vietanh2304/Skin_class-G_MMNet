# import os
# import torch
# import random
# import numpy as np

# class Config:
#     # ============ DATA & PATHS (HAM10000) ===========
#     CSV_FILE = "/home/ibmelab/Documents/skin/skin-cancer-mnist-ham10000/HAM10000_metadata.csv"
#     IMG_ROOTS = [
#         "/home/ibmelab/Documents/skin/skin-cancer-mnist-ham10000/HAM10000_images_part_1", 
#         "/home/ibmelab/Documents/skin/skin-cancer-mnist-ham10000/HAM10000_images_part_2",
#     ]
#     OUTPUT_DIR = "/home/ibmelab/Documents/G_MMNet/checkpoints"
    
#     # ðŸ”¥ KHÃ”NG DÃ™NG RESUME - Train tá»« Ä‘áº§u
#     RESUME_CHECKPOINT = None 
    
#     # ðŸ”¥ THAY Äá»”I: CHá»ˆ Äá»ŠNH GPU 1 Cá»¦A MÃY TRáº M
#     DEVICE = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
    
#     SEED = 42
#     N_SPLITS = 5
    
#     # ============ MODEL ARCHITECTURE (V24 Asymmetric) ===========
#     NUM_CLASSES = 7
#     IMG_SIZE = 224
#     META_DIM = 256
#     IMG_EMBED_DIM_FINE = 192  
#     IMG_EMBED_DIM_COARSE = 768 
#     FUSION_MLP_DIM_FINE = 768    
#     FUSION_MLP_DIM_COARSE = 3072
#     FUSION_NUM_LAYERS = 4
#     FUSION_NUM_HEADS = 8    
#     USE_CROSS_SCALE = True 
    
#     # ============ REGULARIZATION (Giá»¯ nguyÃªn) ===========
#     META_DROPOUT = 0.2
#     FUSION_DROPOUT = 0.2
#     META_FEATURE_DROPOUT_RATE = 0.1
#     MODALITY_DROPOUT_RATE = 0.15
    
#     # ============ TRAINING (Giá»¯ nguyÃªn) ===========
#     BATCH_SIZE = 12
#     EPOCHS = 200 # Cháº¡y tá»‘i Ä‘a 200 (hoáº·c háº¿t 30 tiáº¿ng)
#     PATIENCE = 200 # Táº¯t Early Stop
#     FOLDS_TO_RUN = [0,1,2,3,4]
    
#     # ============ LOSS & STRATEGY (ðŸ”¥ V27 - DÃ™NG FOCAL LOSS) ===========
#     USE_HYBRID_LOSS = False    
#     LABEL_SMOOTHING = 0.0 # âŒ Táº®T Label Smoothing (Focal Loss khÃ´ng cáº§n)
#     USE_FOCAL_LOSS = True # âœ… Báº¬T FOCAL LOSS
#     FOCAL_LOSS_GAMMA = 2.0 # GiÃ¡ trá»‹ tiÃªu chuáº©n
    
#     # ============ OPTIMIZER (Giá»¯ nguyÃªn) ===========
#     WEIGHT_DECAY = 0.05       
#     BETAS = (0.9, 0.999)    
#     EPS = 1e-6
    
#     # ============ LEARNING RATE (V26 - á»”n Ä‘á»‹nh) ===========
#     HEAD_LR = 1e-4            # (0.0002) - Má»©c á»•n Ä‘á»‹nh
#     BACKBONE_LR = 1e-5        
    
#     # ============ SCHEDULER (V26 - á»”n Ä‘á»‹nh) ===========
#     SCHEDULER_TYPE = 'cosine' 
#     WARMUP_EPOCHS = 15        
    
#     # ============ AUGMENTATION (V26 - Báº­t Mixup) ===========
#     USE_TTA = True
#     USE_MIXUP = False
#     MIXUP_PROB = 0.5 
#     MIXUP_ALPHA = 0.4
    
#     # ============ OTHERS (Giá»¯ nguyÃªn) ===========
#     USE_AMP = False         
#     GRAD_CLIP = 0.5  
#     STOCHASTIC_DEPTH_RATE = 0.1       
#     USE_MASKING_LOSS = True   # âœ… Báº­t tÃ­nh nÄƒng Masking Consistency
#     MASKING_RATIO = 0.15      # Giáº£m xuá»‘ng 15% (thay vÃ¬ 25% Ä‘Ã£ thá»­)
#     MASKING_LOSS_WEIGHT = 1.0
# # Khá»Ÿi táº¡o má»™t instance cá»§a Config Ä‘á»ƒ cÃ¡c file khÃ¡c import
# cfg = Config()

# # Táº¡o thÆ° má»¥c output
# os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)

# # In thÃ´ng bÃ¡o xÃ¡c nháº­n
# print(f"\nâœ… Config V27 (Focal Loss + Mixup - Train tá»« Ä‘áº§u) Ready:")
# print(f"   ðŸ”¥ ÄÃƒ CHá»ˆ Äá»ŠNH CHáº Y TRÃŠN: {cfg.DEVICE}")
# print(f"   Model: EffNetB1-Stem + Asymmetric Mamba (192D/768D)")
# print(f"   ðŸ”¥ Chiáº¿n lÆ°á»£c Kappa: Báº¬T Focal Loss (gamma={cfg.FOCAL_LOSS_GAMMA})")
# print(f"   ðŸ”¥ Chiáº¿n lÆ°á»£c Kappa: Báº¬T Mixup/Cutmix (p={cfg.MIXUP_PROB})")
# print(f"   ðŸ”¥ LR: Head={cfg.HEAD_LR}, Backbone={cfg.BACKBONE_LR} (á»”n Ä‘á»‹nh)")
# ===============================================================
# CELL 4: CONFIG - V36 (Single-Stream Bidirectional Mamba)
# ===============================================================
import os
# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True' # Báº­t náº¿u cáº§n
import torch
import random
import numpy as np

class Config:
    # ============ DATA & PATHS (HAM10000) ===========
    CSV_FILE = "/home/ibmelab/Documents/skin/skin-cancer-mnist-ham10000/HAM10000_metadata.csv"
    IMG_ROOTS = [
        "/home/ibmelab/Documents/skin/skin-cancer-mnist-ham10000/HAM10000_images_part_1", 
        "/home/ibmelab/Documents/skin/skin-cancer-mnist-ham10000/HAM10000_images_part_2",
    ]
    OUTPUT_DIR = "/home/ibmelab/Documents/G_MMNet/checkpoints"
    
    # ðŸ”¥ KHÃ”NG DÃ™NG RESUME - Train tá»« Ä‘áº§u
    RESUME_CHECKPOINT = None 
    
    # ðŸ”¥ CHá»ˆ Äá»ŠNH GPU 1
    DEVICE = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
    
    SEED = 42
    N_SPLITS = 5
    
    # ============ MODEL ARCHITECTURE (V36 - Single Stream Lite) ===========
    NUM_CLASSES = 7
    IMG_SIZE = 224            # Giá»¯ 224x224 cho an toÃ n vÃ  nhanh
    META_DIM = 256
    
    # CÃ¡c tham sá»‘ nÃ y khÃ´ng cÃ²n dÃ¹ng cho V36 (vÃ¬ V36 tá»± Ä‘á»‹nh nghÄ©a D_MODEL=512)
    # NhÆ°ng cá»© Ä‘á»ƒ Ä‘Ã¢y Ä‘á»ƒ trÃ¡nh lá»—i náº¿u cÃ³ code cÅ© gá»i tá»›i
    IMG_EMBED_DIM_FINE = 192  
    IMG_EMBED_DIM_COARSE = 768 
    FUSION_MLP_DIM_FINE = 768    
    FUSION_MLP_DIM_COARSE = 3072
    FUSION_NUM_LAYERS = 4
    FUSION_NUM_HEADS = 8    
    USE_CROSS_SCALE = False   # V36 lÃ  Single Stream, khÃ´ng cáº§n Cross Scale
    
    # ============ REGULARIZATION (V36 - á»”n Ä‘á»‹nh) ===========
    META_DROPOUT = 0.2
    FUSION_DROPOUT = 0.2
    META_FEATURE_DROPOUT_RATE = 0.1
    MODALITY_DROPOUT_RATE = 0.15
    STOCHASTIC_DEPTH_RATE = 0.05 # Má»©c ráº¥t nháº¹
    
    # ============ TRAINING (Tá»‘i Æ°u tá»‘c Ä‘á»™) ===========
    BATCH_SIZE = 16       # â¬†ï¸ TÄƒng lÃªn 16 (VÃ¬ V36 nháº¹ hÆ¡n V35)
    EPOCHS = 200 
    PATIENCE = 200
    FOLDS_TO_RUN = [0,1,2,3,4]
    
    # ============ LOSS & STRATEGY ===========
    USE_HYBRID_LOSS = False    
    LABEL_SMOOTHING = 0.0 
    USE_FOCAL_LOSS = True 
    FOCAL_LOSS_GAMMA = 2.0 
    
    # ============ OPTIMIZER ===========
    WEIGHT_DECAY = 0.05       
    BETAS = (0.9, 0.999)    
    EPS = 1e-6
    
    # ============ LEARNING RATE (An toÃ n) ===========
    HEAD_LR = 1e-4            # Giá»¯ má»©c 1e-4 Ä‘á»ƒ trÃ¡nh NaN
    BACKBONE_LR = 1e-5        
    
    # ============ SCHEDULER ===========
    SCHEDULER_TYPE = 'cosine' 
    WARMUP_EPOCHS = 15        
    
    # ============ AUGMENTATION (Táº¯t Mixup Ä‘á»ƒ dÃ¹ng Masking) ===========
    USE_TTA = True
    USE_MIXUP = False 
    MIXUP_PROB = 0.5 
    MIXUP_ALPHA = 0.4
    
    # ============ CONSISTENCY LOSS (V36 - Masking) ===========
    USE_AMP = False           # Táº¯t AMP (FP32) Ä‘á»ƒ trÃ¡nh NaN tuyá»‡t Ä‘á»‘i
    GRAD_CLIP = 0.5           
    USE_MASKING_LOSS = True   # âœ… Báº­t Masking Consistency
    MASKING_RATIO = 0.15      # Má»©c nháº¹ nhÃ ng 15%
    MASKING_LOSS_WEIGHT = 1.0 
    
cfg = Config()

os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)

print(f"\nâœ… Config V36 (Single-Stream Bidirectional + Masking) Ready:")
print(f"   ðŸ”¥ GPU: {cfg.DEVICE}")
print(f"   Model: DenseNet121 -> Single Stream Mamba (512D)")
print(f"   Strategy: Masking Loss (15%) | FP32 | Batch={cfg.BATCH_SIZE}")