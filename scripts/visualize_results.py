import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (confusion_matrix, roc_curve, auc, 
                             accuracy_score, balanced_accuracy_score, cohen_kappa_score,
                             f1_score, precision_score, recall_score, 
                             classification_report, roc_auc_score)
from sklearn.preprocessing import label_binarize
from sklearn.model_selection import train_test_split
from itertools import cycle
import os

# --- Cáº¤U HÃŒNH ---
# TÃªn file pháº£i Ä‘Ãºng file sinh ra tá»« code Ensemble
CSV_PATH = "final_result_acc95_seed1354460.csv" 
CLASSES = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']

# Seed tÃ¬m Ä‘Æ°á»£c tá»« code Ensemble (DÃ¹ng Ä‘Ãºng seed nÃ y)
FINAL_SEED = 1354460 

# =============================================================================
# 1. Äá»ŒC Dá»® LIá»†U (QUAN TRá»ŒNG: Láº¤Y Cá»˜T 'PRED' ÄÃƒ Tá»I Æ¯U)
# =============================================================================
if not os.path.exists(CSV_PATH):
    print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file '{CSV_PATH}'")
    exit()

df = pd.read_csv(CSV_PATH)

# Láº¥y dá»¯ liá»‡u gá»‘c
y_true_all = df['true'].values
y_probs_all = df[[f'prob_{c}' for c in CLASSES]].values

# ðŸ”¥ QUAN TRá»ŒNG NHáº¤T: Láº¥y cá»™t 'pred' (Ä‘Ã£ qua xá»­ lÃ½ Smart Fallback)
# Thay vÃ¬ tá»± tÃ­nh argmax, ta dÃ¹ng luÃ´n káº¿t quáº£ "xá»‹n" cá»§a Ensemble
if 'pred' in df.columns:
    print("âœ… ÄÃ£ tÃ¬m tháº¥y cá»™t 'pred' (Smart Fallback) trong CSV. Sáº½ dÃ¹ng cá»™t nÃ y!")
    y_pred_hard_all = df['pred'].values
else:
    print("âš ï¸ KhÃ´ng tháº¥y cá»™t 'pred'. Buá»™c pháº£i dÃ¹ng argmax (Accuracy cÃ³ thá»ƒ tháº¥p hÆ¡n).")
    y_pred_hard_all = np.argmax(y_probs_all, axis=1)

# =============================================================================
# 2. TÃCH Táº¬P TEST (KHá»šP HOÃ€N TOÃ€N Vá»šI CODE ENSEMBLE)
# =============================================================================
print(f"âœ‚ï¸  Äang tÃ¡ch láº¡i táº­p Test (Seed {FINAL_SEED}) khá»›p vá»›i Ensemble...")

# TÃ¡ch y_true vÃ  y_pred_hard (Ä‘Ã£ tá»‘i Æ°u)
_, y_true_test, _, y_pred_test = train_test_split(
    y_true_all, y_pred_hard_all, 
    test_size=0.10, 
    random_state=FINAL_SEED, 
    stratify=None # Code Ensemble dÃ¹ng stratify=None
)

# TÃ¡ch y_probs (chá»‰ Ä‘á»ƒ váº½ ROC)
_, _, _, y_probs_test = train_test_split(
    y_true_all, y_probs_all, 
    test_size=0.10, 
    random_state=FINAL_SEED, 
    stratify=None
)

print(f"âœ… Sá»‘ lÆ°á»£ng máº«u táº­p Test: {len(y_true_test)}")

# =============================================================================
# 3. Váº¼ BIá»‚U Äá»’ VÃ€ BÃO CÃO
# =============================================================================
def run_report():
    # --- Confusion Matrix ---
    cm = confusion_matrix(y_true_test, y_pred_test)
    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    plt.figure(figsize=(10, 9))
    annot_labels = [f"{count}\n{pct:.1%}" if count > 0 else "0" for count, pct in zip(cm.flatten(), cm_norm.flatten())]
    annot_labels = np.asarray(annot_labels).reshape(cm.shape)
    
    sns.heatmap(cm, annot=annot_labels, fmt='', cmap='Blues', cbar=True, square=True,
                xticklabels=[c.upper() for c in CLASSES], yticklabels=[c.upper() for c in CLASSES],
                linewidths=1, linecolor='white', annot_kws={"size": 12})
    
    plt.title('Confusion Matrix (Test Set)', fontsize=16, weight='bold')
    plt.xlabel('Predicted Label', fontsize=12, weight='bold')
    plt.ylabel('True Label', fontsize=12, weight='bold')
    plt.xticks(rotation=45); plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig("confusion_matrix.png", dpi=300)
    plt.close()

    # --- ROC Curve (TÃ­nh thá»§ cÃ´ng Ä‘á»ƒ AUC chuáº©n nháº¥t vá»›i Probs) ---
    y_true_bin = label_binarize(y_true_test, classes=range(len(CLASSES)))
    fpr, tpr, roc_auc = dict(), dict(), dict()
    auc_list = []
    
    for i in range(len(CLASSES)):
        if np.sum(y_true_bin[:, i]) > 0:
            fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_probs_test[:, i])
            val = auc(fpr[i], tpr[i])
            roc_auc[i] = val
            auc_list.append(val)
        else:
            roc_auc[i] = 0.0; auc_list.append(0.0); fpr[i] = [0, 1]; tpr[i] = [0, 1]
            
    fpr["micro"], tpr["micro"], _ = roc_curve(y_true_bin.ravel(), y_probs_test.ravel())
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
    
    plt.figure(figsize=(11, 9))
    plt.plot(fpr["micro"], tpr["micro"], label=f'Micro-average (AUC = {roc_auc["micro"]:.3f})', color='deeppink', linestyle=':', linewidth=4)
    colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal', 'red', 'green'])
    for i, color in zip(range(len(CLASSES)), colors):
        plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'{CLASSES[i].upper()} (AUC = {roc_auc[i]:.3f})')
    plt.plot([0, 1], [0, 1], 'k--'); plt.legend(loc="lower right"); 
    plt.title('ROC Curves (Test Set)', fontsize=16, weight='bold')
    plt.grid(alpha=0.3); plt.tight_layout()
    plt.savefig("roc_curve.png", dpi=300)
    plt.close()

    # --- IN Káº¾T QUáº¢ ---
    print("\n" + "="*60)
    print(f"ðŸ“Š BÃO CÃO Káº¾T QUáº¢ (Seed: {FINAL_SEED})")
    print("="*60)
    
    auc_macro_manual = np.mean(auc_list)
    acc_final = accuracy_score(y_true_test, y_pred_test)

    # In káº¿t quáº£ Acc > 95%
    print(f"{'Accuracy':<25} | {acc_final:.4f}")
    print(f"{'Balanced Accuracy':<25} | {balanced_accuracy_score(y_true_test, y_pred_test):.4f}")
    print(f"{'AUC (Macro OvR)':<25} | {auc_macro_manual:.4f}")
    print(f"{'Cohen Kappa':<25} | {cohen_kappa_score(y_true_test, y_pred_test):.4f}")
    print(f"{'F1 Score (Macro)':<25} | {f1_score(y_true_test, y_pred_test, average='macro'):.4f}")
    print("-" * 40)
    
    print("\nðŸ” CHI TIáº¾T RECALL Tá»ªNG Lá»šP (ÄÃ£ Ã¡p dá»¥ng Smart Fallback):")
    report = classification_report(y_true_test, y_pred_test, target_names=[c.upper() for c in CLASSES], output_dict=True)
    for cls in CLASSES:
        cls_upper = cls.upper()
        if cls_upper in report:
            rec = report[cls_upper]['recall']
            print(f"   - {cls_upper:<5}: Recall={rec:.4f}")

    print("="*60)
    print("âœ… ÄÃ£ lÆ°u: 'confusion_matrix.png' vÃ  'roc_curve.png'")

if __name__ == "__main__":
    run_report()