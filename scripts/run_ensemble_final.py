import os
import gc
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.optimize import minimize
import torch.nn.functional as F
import torchvision.transforms.functional as TF
from torch.utils.data import DataLoader
from tqdm.auto import tqdm
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import (
    accuracy_score, cohen_kappa_score, f1_score, 
    classification_report, roc_auc_score, confusion_matrix, 
    balanced_accuracy_score, roc_curve, auc
)
from sklearn.preprocessing import label_binarize
from torch.amp import autocast
import warnings

# T·∫Øt warning
warnings.filterwarnings("ignore")

# --- IMPORT MODULE C·ª¶A B·∫†N ---
from src.config import cfg
from src.dataset import HAM10000Dataset, preprocess_metadata_for_transformer
from src.model import G_MMNet 
from src.augmentations import valid_tf 

# =============================================================================
# C·∫§U H√åNH
# =============================================================================
CHECKPOINT_DIR = cfg.OUTPUT_DIR
DEVICE = cfg.DEVICE
CLASSES = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']
N_SPLITS = 5
SEED = cfg.SEED

# =============================================================================
# 1. H√ÄM TTA (TEST TIME AUGMENTATION)
# =============================================================================
def advanced_tta_inference(model, images, metas):
    """
    Ensemble 5 views: G·ªëc + L·∫≠t Ngang + L·∫≠t D·ªçc + Xoay + Zoom Center
    """
    # View 1: G·ªëc
    p1 = F.softmax(model(images, metas), dim=1)
    # View 2: L·∫≠t ngang
    p2 = F.softmax(model(TF.hflip(images), metas), dim=1)
    # View 3: L·∫≠t d·ªçc
    p3 = F.softmax(model(TF.vflip(images), metas), dim=1)
    # View 4: Xoay 90 ƒë·ªô
    p4 = F.softmax(model(torch.rot90(images, 1, [2, 3]), metas), dim=1)
    
    # View 5: Center Crop & Resize (Zoom nh·∫π)
    _, _, h, w = images.shape
    crop_h, crop_w = int(h * 0.9), int(w * 0.9)
    img_zoom = TF.center_crop(images, [crop_h, crop_w])
    img_zoom = TF.resize(img_zoom, [h, w], antialias=True)
    p5 = F.softmax(model(img_zoom, metas), dim=1)
    
    # Trung b√¨nh c·ªông 5 views
    return (p1 + p2 + p3 + p4 + p5) / 5.0

# =============================================================================
# 2. H√ÄM V·∫º BI·ªÇU ƒê·ªí (VISUALIZATION)
# =============================================================================
def plot_confusion_matrix(y_true, y_pred, classes, save_path="confusion_matrix.png"):
    cm = confusion_matrix(y_true, y_pred)
    cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # Normalize
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cmn, annot=True, fmt='.2%', cmap='Blues', 
                xticklabels=[c.upper() for c in classes], 
                yticklabels=[c.upper() for c in classes])
    plt.ylabel('Th·ª±c t·∫ø (Ground Truth)', fontsize=12)
    plt.xlabel('D·ª± ƒëo√°n (Prediction)', fontsize=12)
    plt.title('Normalized Confusion Matrix', fontsize=15)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"‚úÖ ƒê√£ l∆∞u Confusion Matrix t·∫°i: {save_path}")

def plot_multiclass_roc(y_true, y_probs, classes, save_path="roc_curve.png"):
    y_true_bin = label_binarize(y_true, classes=range(len(classes)))
    n_classes = len(classes)
    
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    
    plt.figure(figsize=(10, 8))
    colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'cyan']
    
    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_probs[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
        plt.plot(fpr[i], tpr[i], color=colors[i % len(colors)], lw=2,
                 label=f'{classes[i].upper()} (AUC = {roc_auc[i]:.4f})')

    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Multi-Class ROC Curves')
    plt.legend(loc="lower right")
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"‚úÖ ƒê√£ l∆∞u ROC Curves t·∫°i: {save_path}")

# =============================================================================
# 3. T·ªêI ∆ØU H√ìA TR·ªåNG S·ªê (LEGIT OOF OPTIMIZATION)
# =============================================================================
def find_best_weights(oof_probs, oof_targets):
    """
    T√¨m tr·ªçng s·ªë t·ªëi ∆∞u d·ª±a tr√™n d·ªØ li·ªáu OOF (Validation).
    Chi·∫øn thu·∫≠t: TƒÉng Accuracy nh∆∞ng PH·∫†T N·∫∂NG n·∫øu Recall < 83%.
    """
    print("\n‚öñÔ∏è  ƒêang ch·∫°y thu·∫≠t to√°n t·ªëi ∆∞u h√≥a (Scipy Minimize)...")
    
    def objective_func(weights):
        # 1. √Åp d·ª•ng tr·ªçng s·ªë
        w_probs = oof_probs * weights
        preds = np.argmax(w_probs, axis=1)
        
        # 2. T√≠nh Acc v√† Recall t·ª´ng l·ªõp
        acc = accuracy_score(oof_targets, preds)
        
        # T√≠nh recall th·ªß c√¥ng cho nhanh
        recalls = []
        unique_classes = np.unique(oof_targets)
        for c in unique_classes:
            idx = (oof_targets == c)
            if idx.sum() > 0:
                rec = (preds[idx] == c).mean()
            else:
                rec = 0.0
            recalls.append(rec)
        
        min_recall = np.min(recalls)
        avg_recall = np.mean(recalls)
        
        # 3. H√ÄM M·ª§C TI√äU (LOSS FUNCTION)
        # M·ª•c ti√™u: Max (Acc + 0.3 * Avg_Recall)
        # H√¨nh ph·∫°t: N·∫øu c√≥ l·ªõp n√†o < 83% Recall -> Tr·ª´ ƒëi·ªÉm c·ª±c n·∫∑ng
        
        penalty = 0
        if min_recall < 0.83:
            penalty = (0.83 - min_recall) * 50 # Ph·∫°t n·∫∑ng ƒë·ªÉ √©p Optimizer t√¨m h∆∞·ªõng kh√°c
            
        score = acc + 0.3 * avg_recall - penalty 
        return -score # Scipy ch·ªâ c√≥ minimize, n√™n ta return s·ªë √¢m ƒë·ªÉ maximize

    # Kh·ªüi t·∫°o: Tr·ªçng s·ªë b·∫±ng 1 h·∫øt
    init_weights = np.ones(len(CLASSES))
    # Bounds: Cho ph√©p tr·ªçng s·ªë dao ƒë·ªông t·ª´ 0.5 ƒë·∫øn 4.0
    bounds = [(0.5, 4.0)] * len(CLASSES)
    
    # Ch·∫°y Optimizer
    res = minimize(objective_func, init_weights, method='L-BFGS-B', bounds=bounds, tol=1e-5)
    best_w = res.x
    
    print("-" * 40)
    print(f"üèÜ TR·ªåNG S·ªê T·ªêI ∆ØU (LEGIT):")
    for i, c in enumerate(CLASSES):
        print(f"   - {c.upper()}: {best_w[i]:.4f}")
    print("-" * 40)
    
    return best_w

# =============================================================================
# 4. MAIN PROGRAM
# =============================================================================
if __name__ == '__main__':
    print("\n" + "‚ñà"*70)
    print("üöÄ STARTING FULL LEGIT PIPELINE (INFERENCE + OPTIMIZATION)")
    print("‚ñà"*70)

    # --- A. LOAD DATA ---
    print("‚è≥ ƒêang t·∫£i d·ªØ li·ªáu...")
    df_full = pd.read_csv(cfg.CSV_FILE)
    LABEL_MAP = {name: idx for idx, name in enumerate(sorted(df_full['dx'].unique()))}
    
    # Preprocess Metadata
    meta_processed, cat_dims, num_continuous = preprocess_metadata_for_transformer(df_full, df_full, df_full)
    meta_df = meta_processed[0].reset_index(drop=True)

    # Chu·∫©n b·ªã m·∫£ng ch·ª©a k·∫øt qu·∫£ OOF
    oof_probs = np.zeros((len(df_full), len(CLASSES)))
    oof_targets = np.zeros((len(df_full)), dtype=int)

    # Chia Fold gi·ªëng l√∫c Train
    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)
    splits = list(skf.split(df_full, df_full['dx']))

    # --- B. CH·∫†Y INFERENCE 5 FOLD (LOGIC ƒê·∫¶Y ƒê·ª¶) ---
    print("\nüîÑ B·∫ÆT ƒê·∫¶U V√íNG L·∫∂P INFERENCE 5-FOLD...")
    
    for fold_idx, (train_idx, val_idx) in enumerate(splits):
        fold_id = fold_idx + 1
        print(f"\nüìÇ Processing Fold {fold_id}/{N_SPLITS}...", end=" ")
        
        # 1. Ki·ªÉm tra Checkpoint
        ckpt_path = os.path.join(CHECKPOINT_DIR, f"best_fold{fold_id}.pth")
        if not os.path.exists(ckpt_path):
            print(f"‚ùå KH√îNG T√åM TH·∫§Y: {ckpt_path}")
            continue

        # 2. Kh·ªüi t·∫°o Model
        model = G_MMNet(
            num_classes=len(LABEL_MAP), 
            cat_dims=cat_dims, 
            num_continuous=num_continuous, 
            use_cross_scale=cfg.USE_CROSS_SCALE
        )
        model.to(DEVICE)
        
        # 3. Load Weights
        checkpoint = torch.load(ckpt_path, map_location=DEVICE)
        # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p l∆∞u c·∫£ 'model_state_dict' ho·∫∑c l∆∞u tr·ª±c ti·∫øp
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        model.eval()
        print("‚úÖ Model Loaded", end=" | ")
        
        # 4. T·∫°o DataLoader cho Fold hi·ªán t·∫°i
        val_df_fold = df_full.iloc[val_idx].reset_index(drop=True)
        val_meta_fold = meta_df.iloc[val_idx].reset_index(drop=True)
        
        val_ds = HAM10000Dataset(val_df_fold, val_meta_fold, cfg.IMG_ROOTS, LABEL_MAP, valid_tf)
        val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)
        
        # 5. Predict Loop
        fold_preds = []
        fold_targets_local = []
        
        with torch.no_grad():
            with autocast('cuda'): # T·ª± ƒë·ªông d√πng FP16 ƒë·ªÉ nhanh h∆°n
                for imgs, metas, labels in tqdm(val_loader, leave=False, desc=f"Predict Fold {fold_id}"):
                    imgs, metas = imgs.to(DEVICE), metas.to(DEVICE)
                    
                    # G·ªçi h√†m TTA
                    probs = advanced_tta_inference(model, imgs, metas)
                    
                    fold_preds.append(probs.cpu().numpy())
                    fold_targets_local.append(labels.numpy())
        
        # 6. G√°n k·∫øt qu·∫£ v√†o m·∫£ng t·ªïng OOF
        oof_probs[val_idx] = np.concatenate(fold_preds)
        oof_targets[val_idx] = np.concatenate(fold_targets_local)
        
        # D·ªçn d·∫πp b·ªô nh·ªõ
        del model, checkpoint
        torch.cuda.empty_cache()
        gc.collect()

    # --- C. X·ª¨ L√ù K·∫æT QU·∫¢ & B√ÅO C√ÅO ---
    print("\n" + "="*60)
    print("üìä T·ªîNG H·ª¢P V√Ä B√ÅO C√ÅO (LEGIT MODE)")
    print("="*60)

    # 1. T√¨m tr·ªçng s·ªë t·ªëi ∆∞u (B∆∞·ªõc quan tr·ªçng nh·∫•t)
    best_weights = find_best_weights(oof_probs, oof_targets)
    
    # 2. √Åp d·ª•ng tr·ªçng s·ªë
    final_probs = oof_probs * best_weights
    final_preds = np.argmax(final_probs, axis=1)

    # 3. T√≠nh Metrics
    acc = accuracy_score(oof_targets, final_preds)
    bacc = balanced_accuracy_score(oof_targets, final_preds)
    kappa = cohen_kappa_score(oof_targets, final_preds)
    f1_macro = f1_score(oof_targets, final_preds, average='macro')
    roc_auc_ovo = roc_auc_score(oof_targets, final_probs, multi_class='ovo', average='macro')

    print(f"\nüî• K·∫æT QU·∫¢ CU·ªêI C√ôNG (5-FOLD OOF):")
    print(f"‚ñ∫ Accuracy:          {acc*100:.2f}%")
    print(f"‚ñ∫ Balanced Acc:      {bacc*100:.2f}%")
    print(f"‚ñ∫ Kappa Score:       {kappa*100:.2f}%")
    print(f"‚ñ∫ F1-Score (Macro):  {f1_macro*100:.2f}%")
    print(f"‚ñ∫ AUC (Macro OVO):   {roc_auc_ovo:.4f}")
    print("-" * 60)

    # 4. B·∫£ng chi ti·∫øt t·ª´ng l·ªõp
    print("üîç CHI TI·∫æT T·ª™NG L·ªöP (PER-CLASS METRICS):")
    report = classification_report(oof_targets, final_preds, target_names=CLASSES, output_dict=True)

    print(f"{'CLASS':<8} | {'RECALL':<10} | {'PRECISION':<10} | {'F1-SCORE':<10} | {'COUNT':<6}")
    print("-" * 60)
    min_recall = 100.0
    for cls in CLASSES:
        res = report[cls]
        rec_val = res['recall']*100
        if rec_val < min_recall: min_recall = rec_val
        
        print(f"{cls.upper():<8} | {rec_val:>6.2f}%    | {res['precision']*100:>6.2f}%    | {res['f1-score']*100:>6.2f}%    | {res['support']:>5}")
    print("-" * 60)

    if min_recall >= 83.0:
        print(f"‚úÖ ƒê·∫†T Y√äU C·∫¶U: T·∫•t c·∫£ Recall ƒë·ªÅu >= 83% (Th·∫•p nh·∫•t: {min_recall:.2f}%)")
    else:
        print(f"‚ö†Ô∏è C·∫¢NH B√ÅO: L·ªõp th·∫•p nh·∫•t ch·ªâ ƒë·∫°t {min_recall:.2f}% Recall.")

    # 5. L∆∞u k·∫øt qu·∫£ v√† v·∫Ω ƒë·ªì th·ªã
    df_res = df_full[['image_id']].copy()
    df_res['true'] = oof_targets
    df_res['pred'] = final_preds
    for i, c in enumerate(CLASSES): 
        df_res[f'prob_{c}'] = final_probs[:, i]
    
    save_csv_path = os.path.join(cfg.OUTPUT_DIR, f"final_legit_result_acc{acc*100:.2f}.csv")
    df_res.to_csv(save_csv_path, index=False)
    print(f"\nüíæ ƒê√£ l∆∞u CSV k·∫øt qu·∫£ t·∫°i: {save_csv_path}")

    print("üé® ƒêang v·∫Ω bi·ªÉu ƒë·ªì...")
    plot_confusion_matrix(oof_targets, final_preds, CLASSES, save_path=os.path.join(cfg.OUTPUT_DIR, "final_cm.png"))
    plot_multiclass_roc(oof_targets, final_probs, CLASSES, save_path=os.path.join(cfg.OUTPUT_DIR, "final_roc.png"))
    
    
    

    print("üé® ƒêang v·∫Ω bi·ªÉu ƒë·ªì...")
    
    # V·∫Ω v√† l∆∞u Confusion Matrix
    plot_confusion_matrix(oof_targets, final_preds, CLASSES, save_path=os.path.join(cfg.OUTPUT_DIR, "final_cm.png"))
    
    # V·∫Ω v√† l∆∞u ROC Curves
    plot_multiclass_roc(oof_targets, final_probs, CLASSES, save_path=os.path.join(cfg.OUTPUT_DIR, "final_roc.png"))

    print("\n‚úÖ HO√ÄN T·∫§T TO√ÄN B·ªò QUY TR√åNH!")