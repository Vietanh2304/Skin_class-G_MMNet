nohup: ignoring input

CondaError: Run 'conda init' before 'conda activate'

/home/ibmelab/Documents/G_MMNet/src/augmentations.py:24: UserWarning: Argument(s) 'min_holes, max_holes, min_height, max_height, min_width, max_width' are not valid for transform CoarseDropout
  A.CoarseDropout(min_holes=4, max_holes=8, min_height=cfg.IMG_SIZE//16, max_height=cfg.IMG_SIZE//8, min_width=cfg.IMG_SIZE//16, max_width=cfg.IMG_SIZE//8, p=0.5),
Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.

‚úÖ Config V27 (Focal Loss + Mixup - Train t·ª´ ƒë·∫ßu) Ready:
   üî• ƒê√É CH·ªà ƒê·ªäNH CH·∫†Y TR√äN: cuda:1
   Model: EffNetB1-Stem + Asymmetric Mamba (192D/768D)
   üî• Chi·∫øn l∆∞·ª£c Kappa: B·∫¨T Focal Loss (gamma=2.0)
   üî• Chi·∫øn l∆∞·ª£c Kappa: B·∫¨T Mixup/Cutmix (p=0.5)
   üî• LR: Head=0.0002, Backbone=2e-05 (·ªîn ƒë·ªãnh)
======================================================================
DATA PREPROCESSING (D√ôNG CHO HAM10000)
======================================================================
‚úÖ Data preprocessing (HAM10000) loaded
======================================================================

======================================================================
AUGS: V9 - S·ª≠a l·ªói p=0.5 (Th√™m A.Resize v√†o tr∆∞·ªõc)
======================================================================
‚úÖ Augmentation: train_tf (Resize p=1.0 + RandomResizedCrop p=0.5 ƒê√É S·ª¨A)
‚úÖ Augmentation: valid_tf (Resize)
======================================================================

======================================================================
MODEL V24 - üî• EffNetB1-Stem + Asymmetric Mamba (192D/768D)
======================================================================
======================================================================
‚úÖ MODEL V24 READY - (EffNetB1-Stem + Asymmetric Mamba)
======================================================================

======================================================================
MAIN TRAINING - V27 (Focal Loss + Mixup, cosine LR)
üî• Dataset: HAM10000 | Folds: [0, 1, 2, 3, 4]
üî• CH·∫†Y TR√äN THI·∫æT B·ªä: cuda:1
======================================================================
‚úÖ ƒê√£ t·∫£i 10015 m·∫´u. C√°c l·ªõp: {'akiec': 0, 'bcc': 1, 'bkl': 2, 'df': 3, 'mel': 4, 'nv': 5, 'vasc': 6}

‚úÖ ƒê√£ t·∫°o 5 folds (StratifiedKFold)

================================================================================
üî• B·∫ÆT ƒê·∫¶U FOLD 1/5
================================================================================
  Train: 8012
  Val:   2003
  ‚úÖ Metadata processed (HAM10000):\n     Total features: 5
  ‚úÖ Meta-features: Num=1, Cat=4 [5, 4, 16, 3]
  [Dataset] Created: 8012 samples
  [Dataset] Created: 2003 samples
  ‚úÖ Dataloaders created.
  ‚úÖ ƒê√£ d√πng FocalLoss (gamma=2.0).

============================================================
üî• BUILDING G_MMNet V24 (EffNetB1-Stem + Asymmetric Mamba)
============================================================
  [Dims] Fine: 192D | Coarse: 768D
  [Meta] Encoder: 1 num + 4 cat ‚Üí 256D
  [Stem] EfficientNet-B1 (Pretrained): Fine=40ch, Coarse=112ch
    [Fine] Proj -> 192D
    [Coarse] Proj -> 768D
  [Towers] Building 4 layers √ó 2 streams (Adaptive FiLM)...\n
    [A-FiLM] Adaptive FiLM created (meta + img ‚Üí gamma/beta)
    [A-FiLM] Adaptive FiLM created (meta + img ‚Üí gamma/beta)
    [A-FiLM] Adaptive FiLM created (meta + img ‚Üí gamma/beta)
    [A-FiLM] Adaptive FiLM created (meta + img ‚Üí gamma/beta)
    [A-FiLM] Adaptive FiLM created (meta + img ‚Üí gamma/beta)
    [A-FiLM] Adaptive FiLM created (meta + img ‚Üí gamma/beta)
    [A-FiLM] Adaptive FiLM created (meta + img ‚Üí gamma/beta)
    [A-FiLM] Adaptive FiLM created (meta + img ‚Üí gamma/beta)
    [A-FiLM] Adaptive FiLM created (meta + img ‚Üí gamma/beta)
    [A-FiLM] Adaptive FiLM created (meta + img ‚Üí gamma/beta)
    [A-FiLM] Adaptive FiLM created (meta + img ‚Üí gamma/beta)
    [A-FiLM] Adaptive FiLM created (meta + img ‚Üí gamma/beta)
    [A-FiLM] Adaptive FiLM created (meta + img ‚Üí gamma/beta)
    [A-FiLM] Adaptive FiLM created (meta + img ‚Üí gamma/beta)
    [A-FiLM] Adaptive FiLM created (meta + img ‚Üí gamma/beta)
    [A-FiLM] Adaptive FiLM created (meta + img ‚Üí gamma/beta)
  [Cross-Attn] Dense cross-scale (192D/768D): 4 layers
  [Fusion V24] Advanced Multi-Scale (Chi·∫øu 192D -> 768D)
  [Head] 3-layer: 768‚Üí512‚Üí256‚Üí7
  [Aux Heads] ƒê√£ T·∫ÆT (Thi·∫øt k·∫ø V24)

============================================================
‚úÖ G_MMNet V24 (Asymmetric Pre-trained) READY
============================================================
‚úÖ Optimizer: AdamW (4-Part Smart Weight Decay)
‚úÖ Scheduler: CosineAnnealingLR (Max LR: 0.0002)

   (Validating with x2 TTA: Original + Horizontal Flip...)
/home/ibmelab/anaconda3/envs/gmm_env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
